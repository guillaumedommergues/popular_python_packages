{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which python packages and objects are used the most?\n",
    "\n",
    "Anyone interested in learning Python can easily find hundreds of tutorials on the web. However deciding what to learn, in which order, remains a difficult question. In particular, which packages are popular right now? For instance, to a beginner, the choice between urllib and requests is not obvious.\n",
    "\n",
    "In the same way, most package documentations are organized in an alphabetical order. It's easy to find the documentation for a specific function you know. It's a bit harder to know which functions you must learn first to become efficient in using this package. \n",
    "\n",
    "We want to help to answering these questions by looking at which packages and objects are being used by top Python users now. \n",
    "\n",
    "# Method\n",
    "Specifically, we use the Github API to get the python scripts of the most starred 40 repository for each month between November 2016 and October 2018. We limit ourselves to smaller projects (less than 20 .py files). We parse the scripts (3800 files from 460 repositpries) to localize the imports and their uses. \n",
    "\n",
    "# Resultsa\n",
    "We combine the top packages and objects imported from these packages in the csv file in this repository. \n",
    "\n",
    "For instance, we see that urllib was still used a bit more in the past two years. Also, we see that deep learning packages are extremely popular among highly rated projects.\n",
    "\n",
    "\n",
    "# Requirements\n",
    "Running this Notebook requires Python 3, a github API username and token, both of which are available for free on the Github website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = '<your github username>'\n",
    "token = '<your github token>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to flatten a list - useful in a few places below\n",
    "def flatten(nested_list):\n",
    "    \"\"\" flattens a list\"\"\"\n",
    "    flat_list = []\n",
    "    for sublist in nested_list:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Get a list of python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a function to get the owners and repo names of top repositories in Github for a specific period of time and a specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_followed_repos(start,stop,language,repo_count):\n",
    "    \"\"\"\n",
    "    Get the top x starred github repos in a certain language/time\n",
    "\n",
    "    Args:\n",
    "        start: string yyyy-mm-dd - minimum repo creation date\n",
    "        start: string yyyy-mm-dd - maximum repo creation date\n",
    "        language: string - main language of the repo\n",
    "        repo_count: count of repos in the period (max=100)\n",
    "\n",
    "    Returns:\n",
    "        a list of dictionnaries containing owner and repo name\n",
    "        [{'owner':'abc','repo':'def'}, { } ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    repo_count = min(repo_count,100)  # up to 100 results/page allowed by API\n",
    "\n",
    "    data = requests.get(\n",
    "            'https://api.github.com/search/repositories',\n",
    "            params = {\n",
    "                    'q': 'language:'+language+' created:'+start+'..'+stop,\n",
    "                    'is': 'public',\n",
    "                    'sort':'stars',\n",
    "                    'order':'descending',\n",
    "                    'per_page':repo_count,\n",
    "                    'page':1\n",
    "                    },\n",
    "            auth = (username,token)\n",
    "                    )\n",
    "    data = data.content\n",
    "    data = json.loads(data)\n",
    "\n",
    "    owners_and_names = [item['full_name'].split(\"/\") for item in data['items']]\n",
    "    owners_and_names = [{'owner':x[0],'repo_name':x[1]} for x in owners_and_names]\n",
    "\n",
    "    return owners_and_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that gets the python files paths within that repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_py_files_in_repos(owner,repo):\n",
    "    \"\"\"\n",
    "    Get urls of all python files in a github repository\n",
    "\n",
    "    Args:\n",
    "        owner: string - the username of the repo owner\n",
    "        repo: string - the repository name\n",
    "\n",
    "    Returns:\n",
    "        a list filepaths within repository       \n",
    "    \"\"\"\n",
    "    global data\n",
    "\n",
    "    data = requests.get(\n",
    "        'https://api.github.com/repos/'+owner+'/'+repo+'/git/trees/master',\n",
    "        params = {'recursive':1},\n",
    "        auth = (username,token)\n",
    "    )\n",
    "    data = json.loads(data.content)\n",
    "\n",
    "    all_files = [x['path'] for x in data['tree'] if x['type']== 'blob']\n",
    "    py_files = [x for x in all_files if x[-3:]=='.py']\n",
    "\n",
    "    return py_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's iterate over a some months to find the owners, repo names, and files paths to the python files of the most starred repositories in a period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-497913d6fda5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmost_followed_repos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_starts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstudy_stops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mrepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_followed_repos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmost_followed_repos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_followed_repos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3fea1bb7d426>\u001b[0m in \u001b[0;36mget_most_followed_repos\u001b[0;34m(start, stop, language, repo_count)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mowners_and_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mowners_and_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'owner'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'repo_name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mowners_and_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'items'"
     ]
    }
   ],
   "source": [
    "# create a list of months beginning and ends\n",
    "study_starts = []\n",
    "study_stops = []\n",
    "start = datetime.date(year=2016, month=11, day=1)\n",
    "months = 24\n",
    "for i in range(months):\n",
    "    study_starts.append(datetime.datetime.strftime(start,'%Y-%m-%d'))\n",
    "    start = start + datetime.timedelta(days=40)\n",
    "    start = start.replace(day=1)\n",
    "    stop = start + datetime.timedelta(days=-1)\n",
    "    study_stops.append(datetime.datetime.strftime(stop,'%Y-%m-%d'))\n",
    "    \n",
    "# get the top repositories\n",
    "most_followed_repos = []   \n",
    "for start, stop in zip(study_starts,study_stops):\n",
    "    repos = get_most_followed_repos(start,stop,'python',40)\n",
    "    most_followed_repos = most_followed_repos + repos\n",
    "    \n",
    "# add the python files paths\n",
    "for repo in  most_followed_repos:\n",
    "    repo_name = repo['repo_name']\n",
    "    repo_owner = repo['owner']\n",
    "    try: \n",
    "        repo['files'] = get_py_files_in_repos(repo_owner,repo_name)\n",
    "    except: #some repositories seem to have additional restrictions\n",
    "        repo['files'] = []\n",
    "        \n",
    "# limit ourselves to repositories with fewer than 20 .py files\n",
    "most_followed_repos = [x for x in most_followed_repos if len(x['files'])<=20]\n",
    "most_followed_repos = [x for x in most_followed_repos if len(x['files'])>0]\n",
    "most_followed_repos[0]\n",
    "\n",
    "py_files = flatten([r['files'] for r in most_followed_repos])\n",
    "print('Â§repositories count', len(most_followed_repos))\n",
    "print('files count',len(py_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6137cde4893c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step in this section: we want to see which packages are most often imported. We therefore should exclude local imports. Let's add a list of exceptions (imports to disreguard) which correspond to the local filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an exception list to each dictionaries in most_followed_repos\n",
    "for repo in  most_followed_repos:\n",
    "    repo['exceptions'] = flatten([x[:-3].split('/') for x in repo['files']])\n",
    "\n",
    "most_followed_repos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - parse the extracted files to see which imports are most common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of popular, small, recent Python repositories. We still have to parse them to see which imports are most common.\n",
    "Still using the github API, let's create a function that uses the repo name, owner, and file paths to get the text of a given script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_files_in_repos(owner,repo,filename):\n",
    "    \"\"\"\n",
    "    Get raw scripts of a specific file in specific Guthub repo\n",
    "\n",
    "    Args:\n",
    "        owner: string - the username of the repo owner\n",
    "        repo: string - the repository name\n",
    "        filename: path to the script from repository root\n",
    "\n",
    "    Returns:\n",
    "        a list filepaths within repository       \n",
    "    \"\"\"\n",
    "    \n",
    "    global data\n",
    "\n",
    "    data = requests.get(\n",
    "            'https://api.github.com/repos/'\n",
    "            + owner\n",
    "            + '/'\n",
    "            + repo\n",
    "            + '/contents/'\n",
    "            + filename,\n",
    "            auth = (username,token)\n",
    "            )\n",
    "\n",
    "    data = json.loads(data.content)\n",
    "    if 'content' in data: # not available for one liner files\n",
    "        data = base64.b64decode(data['content'])\n",
    "        data = data.decode('utf-8')\n",
    "    else:\n",
    "        data = ''\n",
    "   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a function that goes through a script and uses some regular expressions to find the imports. Local imports, irrelevant to the study, are excluded. This function is fairly simple. In some very specific cases it may return wrnge results (for instance if the word 'import' is used at the beginning of a line in a docstring). However it does a good job at cathcing all package imports, no local imports, and the few mistakes will not be taken into account as we parse so many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_local_import(from_clause,import_clause,exceptions):\n",
    "    \"\"\"\n",
    "    Identifies the imports in a python script\n",
    "\n",
    "    Args:\n",
    "        from_clause: string - x in \"from x import y\"\n",
    "        from_clause: list of string - y in \"from x import y\"\n",
    "        exceptions: list of strings. filenames to disregard\n",
    "   \n",
    "    Returns:\n",
    "        Boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    if from_clause == ['.']:\n",
    "        return True\n",
    "    \n",
    "    from_clause = from_clause.split('.')\n",
    "    import_clause = [x.split('.') for x in import_clause]\n",
    "    imports = from_clause + flatten(import_clause)\n",
    "    if any([x in exceptions for x in imports]):\n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "def parse_imports(text,exceptions=[]):\n",
    "    \"\"\"\n",
    "    Identifies the imports in a python script\n",
    "\n",
    "    Args:\n",
    "        text: any string. Meant to be raw data of python file\n",
    "        exceptions: string. filename of imports to disregard\n",
    "   \n",
    "    Returns:\n",
    "        a dictionary containing import details\n",
    "        keys: modules/submodules/functions imported\n",
    "        values: how the import will appears in script\n",
    "        \n",
    "    Remark: not 100% accurate.\n",
    "    Would miss a line in the docstring starting with \"import\"for instance\n",
    "\n",
    "    # direct module import\n",
    "    >>> parse_imports('import os')\n",
    "    >>> {'os':'os'}\n",
    "\n",
    "    # direct module import and renaming\n",
    "    >>> parse_imports('import numpy as np')\n",
    "    >>> {'numpy':'np'}\n",
    "\n",
    "    # direct module import and renaming\n",
    "    >>> parse_imports('import numpy as np')\n",
    "    >>> {'numpy':'np'}\n",
    "\n",
    "    >>> parse_imports('from os import listdir,chdir')\n",
    "    >>> {'os.list_dir':'os.list_dir', 'os.chdir':'os.chdir'}\n",
    "\n",
    "    # non identifyable imports\n",
    "    >>> parse_imports('from os import *')\n",
    "    >>> {'os.*':'unidentifiable'}\n",
    "\n",
    "    # excluded local import\n",
    "    >>> parse_imports('from a/utils import script',exceptions=['script'])\n",
    "    >>> {''}\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    statements = text.split('\\n')\n",
    "    for statement in statements:\n",
    "        st = statement\n",
    "        # check if there is a from or import at start of line\n",
    "        if re.search(r'^from |^import ',statement):\n",
    "\n",
    "            # identify and remove \"as clause\" at end of statement\n",
    "            as_clause = re.findall(\n",
    "                    r'. as ([a-zA-Z0-9\\._]+)', statement)\n",
    "            if as_clause:\n",
    "                as_clause = as_clause\n",
    "                statement = statement.split(' as ')[0]\n",
    "\n",
    "            # identify and remove \"from clause\" at end of statement\n",
    "            from_clause = re.findall(\n",
    "                    r'^from ([a-zA-Z0-9\\._]+) import', statement)\n",
    "            \n",
    "            if from_clause:\n",
    "                from_clause = from_clause[0]\n",
    "                statement = statement.split(' import ')[1]\n",
    "            else:\n",
    "                statement = statement.split('import ')[1]\n",
    "                from_clause = ''\n",
    "\n",
    "            # identify \"import clause\" at end of statement\n",
    "            import_clause = re.split(r' *,+',statement)\n",
    "            import_clause = [x.strip() for x in import_clause]\n",
    "\n",
    "            # check if statement is local import\n",
    "            if is_local_import(from_clause,import_clause,exceptions):\n",
    "                pass\n",
    "            else:\n",
    "                if import_clause[0] == '*':\n",
    "                    as_clause = ['unidentifiable']\n",
    "\n",
    "                # add from clause to each import - if applicable\n",
    "                if from_clause:\n",
    "                    imported = [from_clause + '.' + imp for imp in import_clause]\n",
    "                else:\n",
    "                    imported = import_clause\n",
    "\n",
    "                # add as to imports whose names changed - if applicable\n",
    "                if as_clause:\n",
    "                    imported_as = as_clause\n",
    "                else:\n",
    "                    imported_as = import_clause\n",
    "\n",
    "                for imp,imp_as in zip(imported,imported_as):\n",
    "                    imp = imp.strip()\n",
    "                    imp_as = imp_as.strip()\n",
    "                    results[imp] = imp_as\n",
    "                    if imp in ['ose()','os()','ose'] or imp_as in ['ose()','os()','ose']:\n",
    "                        print(st)\n",
    " \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another function that goes through a script, and uses the imports to see how often and how they are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_script(imports, text):\n",
    "    \"\"\"\n",
    "    Identifies how imports in a python script are used\n",
    "\n",
    "    Args:\n",
    "        imports: a dictionary - key is the import, value how it will appear\n",
    "        text: a string. Meant to be raw data of python file\n",
    "\n",
    "    Returns:\n",
    "        a Counter object with import usage\n",
    "        keys: modules/submodules/functions imported\n",
    "        values: specific instances or functions used\n",
    "\n",
    "    # function from module\n",
    "    >>> parse_script({'os':'os'},\"os.listdir('.')\"\n",
    "    >>> Counter({'os.listdir':1})\n",
    "\n",
    "    # function\n",
    "    >>> parse_script({'flask.render_template':'flask.render_template'},\n",
    "    \"render_template('main.html')\"\n",
    "    >>> Counter({'flask.render_template':1})\n",
    "\n",
    "    # instance\n",
    "    >>> parse_script({'sklearn.linear_model':'linear_model'},\n",
    "    \"regr = linear_model.LinearRegression()\"\n",
    "    >>> Counter({'sklearn.linear_model.LinearRegression':1})\n",
    "\n",
    "    # non identifyable import\n",
    "    >>> parse_imports({'os.*':'unidentifiable'},'listdir('.')')\n",
    "    >>> {'os.*':'unidentifiable'}\n",
    "    \"\"\"\n",
    "    \n",
    "    results = Counter()\n",
    "\n",
    "    for key in imports:\n",
    "        appears_as = imports[key]\n",
    "        if appears_as != 'unidentifiable':\n",
    "            regex_string = r'[\\s+-=*/]' + re.escape(appears_as) + '([\\.\\w]*\\()'\n",
    "            matches = re.findall(regex_string,text)\n",
    "            matches = [key + match + ')'for match in matches]\n",
    "            matches = Counter(matches)\n",
    "            results = results + matches\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate over our list of repos and screen each file for most common imports. The results will be stored in a Counter object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = Counter()\n",
    "for d in most_followed_repos:\n",
    "    for script in d['files']:\n",
    "        try:\n",
    "            raw = parse_files_in_repos(d['owner'],d['repo_name'],script)\n",
    "            imports = parse_imports(raw,exceptions=d['exceptions'])\n",
    "            import_use = parse_script(imports, raw)\n",
    "            results = results + import_use\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the data collected to see which packages are imported the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_packages = []\n",
    "for key in results:\n",
    "    most_common_packages = most_common_packages + [key.split('.')[0]]*results[key]\n",
    "most_common_packages = Counter(most_common_packages)\n",
    "most_common_packages.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or within each package, which submodules or functions are most commonly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for package in most_common_packages.most_common(20):\n",
    "    p_name = package[0]\n",
    "    package_functions= [x for x in results.elements() if x.split('.')[0]==p_name]\n",
    "    package_functions = Counter(package_functions)\n",
    "    df[p_name] = ([x[0] for x in package_functions.most_common(10)] + ['']*10)[0:10]\n",
    "\n",
    "df.to_csv('popular_python_packages.csv')\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
